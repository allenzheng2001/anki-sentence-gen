{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3c830cf8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Exported 50 flashcards to generated_flashcards.apkg\n"
     ]
    }
   ],
   "source": [
    "import genanki\n",
    "\n",
    "# === Parameters ===\n",
    "deck_id = 2059400110  # large random int\n",
    "model_id = 1607392319\n",
    "deck_name = \"Generated Flashcards\"\n",
    "output_filename = \"generated_flashcards.apkg\"\n",
    "num_cards = 50\n",
    "\n",
    "# === Define the model (Basic front/back) ===\n",
    "my_model = genanki.Model(\n",
    "    model_id,\n",
    "    'Basic Model',\n",
    "    fields=[\n",
    "        {'name': 'Front'},\n",
    "        {'name': 'Back'},\n",
    "    ],\n",
    "    templates=[{\n",
    "        'name': 'Card 1',\n",
    "        'qfmt': '{{Front}}',\n",
    "        'afmt': '{{FrontSide}}<hr id=\"answer\">{{Back}}',\n",
    "    }]\n",
    ")\n",
    "\n",
    "# === Create the deck ===\n",
    "my_deck = genanki.Deck(\n",
    "    deck_id,\n",
    "    deck_name\n",
    ")\n",
    "\n",
    "# === Generate flashcards ===\n",
    "for i in range(num_cards):\n",
    "    front = f\"FRONTFILLER_{i}\"\n",
    "    back = f\"BACKFILLER_{i}\"\n",
    "    note = genanki.Note(\n",
    "        model=my_model,\n",
    "        fields=[front, back]\n",
    "    )\n",
    "    my_deck.add_note(note)\n",
    "\n",
    "# === Export to .apkg ===\n",
    "genanki.Package(my_deck).write_to_file(output_filename)\n",
    "\n",
    "print(f\"‚úÖ Exported {num_cards} flashcards to {output_filename}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "247bc865",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üñ•Ô∏è GPU Status:\n",
      "Name\t\tTotal Mem\tUsed Mem\tUtilization\n",
      "NVIDIA GeForce RTX 4060 Laptop GPU, 8188, 0, 0\n",
      "üìä Model: None\n",
      "‚öôÔ∏è Backend: Unknown\n",
      "‚ùÑÔ∏è CPU or fallback backend in use\n",
      "\n",
      "üß† Sending prompt to 'mistral'...\n",
      "üì§ Prompt: Translate this sentence into Japanese with furigana: 'I want to learn how to cook.'\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import json\n",
    "import time\n",
    "import subprocess\n",
    "\n",
    "# === Configuration ===\n",
    "MODEL = \"mistral\"\n",
    "OLLAMA_URL = \"http://localhost:11434\"\n",
    "\n",
    "def print_gpu_info():\n",
    "    \"\"\"Print current GPU status using nvidia-smi\"\"\"\n",
    "    try:\n",
    "        output = subprocess.check_output([\n",
    "            'nvidia-smi', '--query-gpu=name,memory.total,memory.used,utilization.gpu',\n",
    "            '--format=csv,nounits,noheader'\n",
    "        ])\n",
    "        print(\"üñ•Ô∏è GPU Status:\")\n",
    "        print(\"Name\\t\\tTotal Mem\\tUsed Mem\\tUtilization\")\n",
    "        print(output.decode().strip())\n",
    "    except Exception as e:\n",
    "        print(\"‚ö†Ô∏è Could not access GPU via nvidia-smi:\", e)\n",
    "\n",
    "def get_model_info(model=MODEL):\n",
    "    \"\"\"Query Ollama to confirm model and backend\"\"\"\n",
    "    url = f\"{OLLAMA_URL}/api/show\"\n",
    "    try:\n",
    "        response = requests.post(url, json={\"name\": model})\n",
    "        if response.ok:\n",
    "            data = response.json()\n",
    "            backend = data.get(\"details\", {}).get(\"backend\", \"Unknown\")\n",
    "            print(f\"üìä Model: {data.get('name')}\")\n",
    "            print(f\"‚öôÔ∏è Backend: {backend}\")\n",
    "            if \"cuda\" in backend.lower():\n",
    "                print(\"üî• GPU is active (CUDA confirmed)\")\n",
    "            else:\n",
    "                print(\"‚ùÑÔ∏è CPU or fallback backend in use\")\n",
    "        else:\n",
    "            print(\"‚ùå Model query failed:\", response.status_code, response.text)\n",
    "    except Exception as e:\n",
    "        print(\"‚ö†Ô∏è Error querying model:\", e)\n",
    "\n",
    "def call_ollama(prompt, model=MODEL):\n",
    "    \"\"\"Send prompt to Ollama and log status\"\"\"\n",
    "    url = f\"{OLLAMA_URL}/api/generate\"\n",
    "    payload = {\n",
    "        'model': model,\n",
    "        'prompt': prompt,\n",
    "        'stream': False\n",
    "    }\n",
    "\n",
    "    print(f\"\\nüß† Sending prompt to '{model}'...\")\n",
    "    print(\"üì§ Prompt:\", prompt[:100] + \"...\" if len(prompt) > 100 else prompt)\n",
    "\n",
    "    start = time.time()\n",
    "    response = requests.post(url, json=payload)\n",
    "    duration = time.time() - start\n",
    "\n",
    "    print(f\"üì° HTTP Status: {response.status_code}\")\n",
    "    if response.ok:\n",
    "        try:\n",
    "            data = response.json()\n",
    "            if 'error' in data:\n",
    "                print(f\"‚ùå Ollama Error: {data['error']}\")\n",
    "            else:\n",
    "                output = data['response']\n",
    "                print(f\"‚úÖ Response in {duration:.2f} sec:\")\n",
    "                print(output.strip())\n",
    "                return output.strip()\n",
    "        except Exception as e:\n",
    "            print(\"‚ö†Ô∏è Could not parse JSON:\", e)\n",
    "            print(\"üîç Raw response:\")\n",
    "            print(response.text)\n",
    "    else:\n",
    "        print(f\"‚ùå Request failed: {response.status_code}\")\n",
    "        print(\"üîç Raw error:\")\n",
    "        print(response.text)\n",
    "\n",
    "# === Run Diagnostic + Inference ===\n",
    "print_gpu_info()\n",
    "get_model_info()\n",
    "call_ollama(\"Translate this sentence into Japanese with furigana: 'I want to learn how to cook.'\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
